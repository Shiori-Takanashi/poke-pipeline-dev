# POKE-PIPELINE-DEV

POKE-PIPELINE-DEVは、ポケモンのデータを対象としたETL（Extract-Transform-Load）パイプラインプロジェクトです。

Pythonによる非同期処理を活用し、**PokéAPI**（ポケモンの公開RESTful API）から大量のデータを効率的に取得・加工・格納することを目的としています。

エンジニアや採用担当者への技術ポートフォリオとして、設計思想やコード構成、使用方法を明確に示すよう構成されています。

## プロジェクト概要 (目的・概要・前提)

* **目的:**

  - ポケモンの公式APIからデータを抽出し、扱いやすい形式で蓄積することで、データ分析やアプリケーション開発に活用できる基盤を構築すること。

  - 本プロジェクトは就職活動向けのポートフォリオとして位置づけられており、設計から実装・テストまでの一連のプロセスを示します。

* **概要:**

  - PokéAPI（[https://pokeapi.co](https://pokeapi.co)）が提供するポケモンに関する各種リソース（ポケモン、フォーム、種族、タイプ、ゲームバージョン等）のデータをETLパイプラインで収集します。

  - APIからデータを**抽出 (Extract)**し、必要に応じて**変換 (Transform)**・整理を行い、ローカルのデータベースやファイルに**格納 (Load)**します。

  - 特に**非同期処理**による高速なデータ取得と、取得データの構造化保存に重点を置いています。

* **前提:**

  - 開発言語はPython 3系。

  - PokéAPIへのアクセスにはインターネット接続が必要。

  - ポートフォリオ用途のため、コード品質（型ヒントやリント、テスト）や再現性（Poetryによる環境構築）にも配慮。

  - データは公式APIから取得しますが、ポケモンに関する知的財産は任天堂等に帰属するため、本プロジェクトは非公式・個人開発のものであり、取得したデータは研究目的で使用します。

## 技術スタック（主要ライブラリ・設計方針・アーキテクチャ）

* **言語・実行環境:**

  - Python 3.x系 （開発時にPython 3.10で動作確認）。

  - パッケージ管理にはPoetryを使用し、依存関係や仮想環境を管理します。

* **APIクライアント:**

  - `aiohttp` ライブラリを用いた**非同期HTTP通信**でPokéAPIからデータを取得。

  - `asyncio`のイベントループ上で複数のAPIリクエストを同時並行で送り、待ち時間を短縮。

  - 例えば50件のリクエストを同時発行し、全てのポケモンデータ（現在約1000件）を効率良く取得（**セマフォで最大50並列に制限**）。

* **データ取得・加工:**

  - 応答データはJSON形式で受け取り、必要に応じて**リトライ**処理（一定回数失敗時の再試行）や**タイムアウト**管理（応答が遅い場合の中断）を組み込み、信頼性を高めています。

  - 取得したJSONデータはそのままファイルに保存する他、Pythonオブジェクトとして処理して**データベース挿入**などの変換を行います。

  - エンドポイント名にハイフンが含まれるもの（例: "pokemon-form"）はプログラム内で扱いやすいよう**リネーム**（アンダースコアに置換）してあります。

* **データベース:**

  - ローカルでSQLiteデータベース（ファイル: `poke.sqlite3`）を使用。

  - 標準ライブラリの`sqlite3`および非同期処理用に`aiosqlite`をインストールしており、取得したポケモンデータを構造化して格納するのに利用します。

  - 将来的にデータ量やアクセス需要に応じてPostgreSQL等への移行も想定し、ORMとして`SQLAlchemy`も導入済み。

* **データモデル:**

  - データ構造の定義やバリデーションには`pydantic`を使用予定。

  - 外部ツールの**datamodel-code-generator**でPokéAPIのスキーマからモデルクラスを自動生成し、データ整形に役立てます。

  - JSONからPythonオブジェクトへのマッピングが容易になり、後段の処理（データベース保存や分析）をシンプルにします。

* **CLIインターフェース:**

  - ターミナルからパイプラインを実行・操作できるよう、`click`と`InquirerPy`を導入。

  - 例えば将来的に`poetry run poke-pipeline fetch`のようなコマンドでデータ取得を開始したり、対話的に取得対象を選択する機能の実装を検討（※現時点では実装中段階）。

* **その他ライブラリ:**

  - 開発効率とコード品質のために以下を使用:

    - フォーマッタ: **black**（コード整形）、**isort**（インポート整理）

    - リンター・型チェック: **flake8**や**mypy**（静的型検査によるバグ予防）

    - テストフレームワーク: **pytest**（および`pytest-asyncio`による非同期関数のテスト）

    - ログ・UI: **rich**（カラフルなログ出力や進行状況プログレスバー表示に使用検討）、**tqdm**（進行バーライブラリ、現在はコンソール出力に利用）

    - JSONユーティリティ: **genson**（複数JSONからスキーマ生成に使用予定）、**PyYAML**（設定ファイルやデータのYAML出力用にインストール済み）

    - Webフレームワーク: **Flask**（将来的に収集データを閲覧する簡易Webアプリ/APIを構築することも視野に入れています）

* **アーキテクチャ設計:**

  - このプロジェクトは**モジュール分離**と**設定の集中管理**を意識。

  - 抽出・変換・格納の各処理を別モジュールに分け、再利用性とテスト容易性を高めています。

  - APIのURLや取得対象リスト、タイムアウト等の**設定値は`config`モジュール**に集約し、コード中にベタ書きしない方針。

  - 非同期処理による並列データ取得と、ローカルファイル/DBへの保存という**パイプライン構造**を持ち、処理フロー全体を通して例外処理やリソース解放（HTTPセッションやDB接続のクローズ）にも注意を払っています。

## ディレクトリ構成と各モジュールの説明

プロジェクトのディレクトリツリーは以下の通りです（`poke-pipeline-dev`リポジトリ内）。各フォルダ/モジュールの役割を簡単に説明します。

```plaintext
poke-pipeline-dev/
├── config/                 # 設定関連のモジュール群
│   ├── constant.py         # グローバル定数定義 (BASE_URLやTIMEOUT値など)
│   ├── dirpath.py          # ディレクトリパス定義 (出力JSONやDBファイルのパス)
│   └── target.py           # 取得対象のエンドポイント定義 (FETCH_TARGETリスト 等)
├── debug/                  # デバッグ用スクリプト
│   └── debug_fetch_target.py   # 取得処理のデバッグ実行用スクリプト
├── database/               # データベースファイル置き場 (Git管理対象外)
│   └── poke.sqlite3            # SQLiteデータベース本体 (生成されるファイル)
├── dev/                    # 開発支援用スクリプト類
│   └── tree.py                 # プロジェクト構成をツリー表示するスクリプト (tree.txt生成)
├── out/                    # 出力用ディレクトリ (ログや生成物)
│   └── tree.txt                # プロジェクト構成のツリー出力結果 (ドキュメント用)
├── json/                   # 取得したJSONデータの保存先 (各エンドポイント毎のサブフォルダ)
│   ├── pokemon/               # ポケモン各個体のJSON (例: 00001.json, 00002.json, ...)
│   ├── pokemon_form/          # ポケモンのフォームデータのJSON
│   ├── pokemon_species/       # ポケモンの種族データのJSON (進化情報等を含む)
│   ├── type/                  # ポケモンのタイプデータのJSON (各タイプの相性等)
│   ├── version/               # ポケモンゲームのバージョンデータのJSON (赤・青・金...など)
│   └── version_group/         # ゲームバージョングループのJSON (例: 第○世代グループ)
├── meta/                   # メタデータ・中間生成物
│   ├── map_endpoints.json     # エンドポイント一覧のマッピングJSON (PokéAPI全リソースリスト)
│   └── map_form.json          # ポケモンフォームに関するマッピングJSON (フォームIDとポケモン対応 等)
├── src/                    # メインのソースコード
│   ├── fetch/                 # 抽出（データ取得）処理に関するパッケージ
│   │   ├── fetch_endpoints.py     # APIの全エンドポイント一覧を取得する処理
│   │   └── fetch_target.py        # 指定した対象データを全件取得する処理（非同期処理の実装）
│   ├── transform/             # ※将来的な変換処理用（現状存在すればスタブ）
│   ├── load/                  # ※将来的なロード処理用（SQLite挿入処理などを想定）
│   └── __init__.py            # パッケージ初期化（srcをモジュールパスに含めるための空ファイル）
├── tests/                  # テストコード
│   ├── test_fetch_target.py    # データ取得モジュール(fetch_target)のテスト
│   ├── test_paths_json.py      # JSONファイル出力周りのテスト（パスやファイル名の検証）
│   └── test_re_endpoint_name.py# 文字列変換ユーティリティのテスト（エンドポイント名リネーム）
├── util/                   # 汎用ユーティリティモジュール
│   ├── directory.py           # ディレクトリ作成・管理ユーティリティ (必要なフォルダの自動生成など)
│   └── re_endpoint_name.py    # エンドポイント名の正規化ユーティリティ ("pokemon-form"→"pokemon_form"等)
├── pyproject.toml          # （Poetryのプロジェクト定義ファイル。依存関係やメタ情報を記載）
├── requirements.txt        # 依存パッケージ一覧（Poetry未使用者向けにエクスポートしたもの）
├── requirements-test.txt   # テスト実行に必要な追加パッケージ一覧（pytest等）
├── .gitignore              # Git管理除外ファイル設定（jsonディレクトリやDBファイル等を除外）
└── README.md               # プロジェクトの説明ドキュメント（本書）
```

* **config/**：設定値を集約したモジュール群です。`constant.py`にはAPIのベースURLやタイムアウト・リトライ回数などの定数、`target.py`には取得対象とするリソース名のリスト（例: `FETCH_TARGET = ["pokemon", "pokemon-form", ...]`）が定義されています。`dirpath.py`ではプロジェクト内で使用するパスをPathオブジェクトで定義しています（例: JSON保存先のルートディレクトリ`JSON_DIR_PATH`など）。これらを切り出すことで**コード中に直接パス文字列やURLを書かず**に済み、メンテナンス性が向上します。

* **debug/**：デバッグ用途のスクリプトを配置します。`debug_fetch_target.py`はデバッグ実行用で、設定値や取得対象一覧を出力し、`get_name_and_url()`関数で実際に取得する各エンドポイントの名前とURLのペア一覧を表示します。開発中に処理の一部を手軽に検証するためのものです。

* **database/**：SQLiteデータベースファイルを置くディレクトリです。`poke.sqlite3`という名前でデータベースファイルを作成します（gitでは`.gitignore`により追跡対象外）。現在はポケモンデータを格納するテーブル（例: `pokemon`テーブル等）をこのファイルに保持します。簡易にデータを参照・クエリできるようにするための仕組みです。

* **dev/**：開発補助用のスクリプトを置くフォルダです。`tree.py`はプロジェクトのディレクトリ構造を自動的にテキスト出力するユーティリティで、READMEに記載するツリー図（上記）を更新する際に使用しました。その他、将来的に使い捨てのデータ加工スクリプトや移行スクリプトなどを配置することも想定しています。

* **out/**：成果物やログの出力先ディレクトリです。現状は`tree.txt`（`tree.py`の出力）を格納しています。今後、データ解析結果のレポートやグラフ、ログファイルなど、最終的な出力物があればここに配置する予定です。

* **json/**：PokéAPIから取得した\*\*生データ(JSON)\*\*を保存するディレクトリです。各サブフォルダがエンドポイント種別を表しており、例えば`json/pokemon/00001.json`には図鑑番号1番のポケモン（フシギダネ）のデータ、`json/type/00018.json`にはタイプ18番（フェアリータイプ）のデータが入ります。ファイル名は`"{idを5桁ゼロ埋め}.json"`形式で、取得した全件をID順に格納します（例: ポケモンは001～1010.jsonまで）。なお、この`json`ディレクトリはリポジトリには含めず（`.gitignore`で除外）、ユーザが実行した際に生成される運用データとして扱います。

* **meta/**：APIから取得したメタ情報や手動作成したマッピングデータを置くフォルダです。`map_endpoints.json`にはPokéAPIの提供する全エンドポイント一覧がJSON形式で記録されています（キーがエンドポイント名、値がURL）。`map_form.json`にはポケモンのフォームに関する対応関係データが格納されています。例えば、あるポケモン種（ピカチュウなど）に対して存在するフォーム（色違いやメガシンカ等）IDの一覧など、フォームデータを整理するための情報です。これらのファイルはデータ変換時に参照し、**データ同士の関連付け**（例: ポケモン基本データとフォームデータの統合表示）に役立てます。

* **src/**：本プロジェクトの主要なソースコードを含むパッケージです。責務ごとにサブパッケージを切っています。

  * `src/fetch/`：データ抽出（取得）に関する処理。**APIからデータを集める部分**です。`fetch_endpoints.py`ではPokéAPIのベースURL（`/api/v2/`）にアクセスし、利用可能なエンドポイント一覧を取得・表示する関数を提供しています。`fetch_target.py`では、`config.target`で指定された各リソースについて全件データを取得する処理を実装しています。この中でHTTPリクエストの並列処理やリトライ制御などのロジックを持っています。例えば`fetch_target.py`内の`fetch_all()`関数は与えられた複数のURLに対して非同期にアクセスし、各データを取得・保存する一連の処理をまとめています（**ETLの「抽出」フェーズ**）。

  * `src/transform/`：データ変換に関する処理。**ETLの「変換」フェーズ**を担当します（現在の実装では最低限の処理のみで、詳細なデータ加工は今後の予定です）。将来的には、取得したJSONデータをPydanticモデルにマッピングしたり、不要なフィールドの除去・リネーミング、他エンドポイントからのデータ参照（例: ポケモン種族データに含まれる進化チェーン情報を辿ってツリー構造化する等）を行う予定です。

  * `src/load/`：データ格納に関する処理。**ETLの「ロード」フェーズ**で、整形済みデータをデータベース（SQLite）に投入したり、必要に応じて別フォーマット（CSVやParquet）で出力する機能を実装予定です。現時点ではSQLiteへの簡単なINSERT処理やテーブル定義を行う関数のプロトタイプがある段階です。将来的にSQLAlchemyを用いたORMモデル定義を行い、Pythonオブジェクトとしてデータベースを扱えるようにする計画です。

  * その他`src/__init__.py`はパッケージ定義のための空ファイルです。

    ※現在、`transform`や`load`ディレクトリ内に具体的な実装コードはほとんどなく、主に`fetch`内で取得と同時に最低限の加工・保存を行っています。プロジェクト拡大に伴い、これらを適切に移譲していく予定です。

* **tests/**：Pytestによる単体テストコードを配置しています。主要な機能ごとにテストモジュールを用意し、想定通りに動作するかを検証します。例えば、`test_fetch_target.py`では`get_name_and_url()`や非同期処理`fetch_json_with_retry()`などの関数が正しく結果（名前とURLのリストや、API応答JSON）を返すか確認しています。また`pytest.mark.asyncio`を用いて非同期関数をテストしています。`test_re_endpoint_name.py`では、エンドポイント名のリネーム関数について、"pokemon-form"という入力が"pokemon\_form"に正規化されるか、といった挙動をチェックします。テストは`poetry run pytest`で実行でき、継続的インテグレーションや将来のリファクタリングに備えています。

* **util/**：汎用的な補助関数を含むモジュールです。`directory.py`には、必要なディレクトリ構造を事前に作成する`setup_dir()`関数等があり、パイプライン実行前に呼び出すことで`json/`下の各フォルダや`database/`フォルダが存在しない場合に自動生成します。`re_endpoint_name.py`にはエンドポイント名からPythonの識別子やパスとして適切な名前へ変換する関数（例: ハイフンをアンダースコアに置換する`rename_endpoint_name()`）が定義されています。これらのユーティリティにより、本来処理の主目的ではない環境準備や文字列処理のコードを本流から切り離し、可読性と再利用性を向上させています。

* **pyproject.toml**：Poetryによるプロジェクト設定ファイルです。プロジェクト名やバージョン、依存パッケージ（ライブラリ）一覧、スクリプトエントリポイントなどが記載されています。Poetryを使うことで、依存関係の追加やプロジェクトのビルド/公開が簡易になります（本プロジェクトではPyPI公開は予定していませんが、Poetryによる環境管理を前提としています）。

* **requirements.txt / requirements-test.txt**：Poetry非利用の場合に備えて、依存パッケージをリスト化したテキストファイルです。`requirements.txt`には本プロジェクトの動作に必要なすべてのパッケージ（`aiohttp`や`SQLAlchemy`等）が、`requirements-test.txt`にはテスト実行のための追加パッケージ（`pytest`や`pytest-asyncio`等）が含まれます。通常はPoetryで環境構築するため直接編集は不要ですが、トラブルシューティングや特定環境下でPoetryが使えない場合にこのファイルを使用してpipインストールできます。

* **README.md**：プロジェクトの説明ドキュメントです。使用技術やモジュールの説明、使い方などを記載しています（まさに本書がそれです）。

## 処理フロー（ETL全体の動作の流れ）

このセクションでは、本パイプラインがデータを取得し、保存するまでの**一連の流れ**を解説します。ETLの各ステップ（抽出・変換・格納）に沿って説明します。

1. **抽出 (Extract) - APIからのデータ取得:**

   パイプラインはまずPokéAPIからデータを取得します。`src/fetch/fetch_target.py`内のメイン関数である`get_name_and_url()`を呼び出すことで、**対象リソースの一覧**（名前とエンドポイントURLのペア）を取得します。例えば、対象に"pokemon"と"pokemon-form"が指定されていれば、それぞれのベースURL（`https://pokeapi.co/api/v2/pokemon/` 等）とリソース名をリストで受け取ります。

   次に、`fetch_all(urls, names)`関数で本格的なデータ取得処理を開始します。この関数は与えられた複数のAPIエンドポイントに対し、**非同期リクエスト**を並行して投げる処理を行います。Pythonの`asyncio`を用いており、内部では`asyncio.gather`等で同時に多数のリクエストを処理しています。ただし、一度に無制限にリクエストするとAPIサーバに負荷をかけるため、`asyncio.Semaphore`によって同時実行数を例えば50に制限しています。50件のリクエストが完了次第次のリクエストに進む仕組みで、約1000体のポケモンデータも短時間で取得可能です。

   **リトライとタイムアウト:** 各リクエストは`fetch_json_with_retry(url, ...)`という非同期関数で実行され、HTTPステータスがエラーの場合や接続が失敗した場合には指定回数（デフォルト5回）まで再試行します。また、各リクエストにはタイムアウト（デフォルト10秒）を設定し、応答がない場合はキャンセルして次の再試行に切り替えます。これにより、一部のネットワーク不良や一時的なAPI不調があってもパイプライン全体が停止せずに完走するよう設計しています。

   **データ件数の判定:** あるリソースに何件のデータが存在するかは、PokéAPIの各リソース一覧エンドポイントで取得できる`count`フィールドや結果リストによって判定します。`fetch_endpoint_for_idxes(url, name, ...)`関数内で、まず各リソースの一覧を取得し、その`count`分だけIDを生成して個別データを取りに行きます（PokéAPIでは`GET /pokemon/25/`のようにID指定で詳細データを取得可能なため、1からcountまでループします）。一覧取得の際にもlimitパラメータを活用して全件取得し、一度のリクエストでID一覧を把握しています。

2. **変換 (Transform) - データ変換・加工:**

   抽出したJSONデータは基本的にPokéAPIのレスポンスそのままですが、パイプライン内でいくつかの軽微な加工を行います。例えば、**エンドポイント名の正規化**です。PokéAPIには「pokemon-form」のようにハイフンを含む名前がありますが、これをそのままフォルダ名やPythonの変数名に使うと不都合があります。そのため、`util/re_endpoint_name.py`モジュールで`pokemon-form`を`pokemon_form`に置換する処理を行い、ファイルパスやデータベースのテーブル名などに利用しています。

   また、**関連データのマッピング**も変換工程の一部です。例えば、ポケモン種族データには進化関係を示す`evolution_chain`のURLが含まれています。将来的にはこのURLから進化チェーンの詳細データを取得し、種族データに統合する予定です。同様に、各ポケモンデータにはフォーム違いの情報が含まれる場合があり、`meta/map_form.json`を用いてどのポケモンにどんなフォームIDがあるかを整理し、基本ポケモンデータとフォームデータを結合する変換を検討しています。

   さらに、一部の重複情報削減やフォーマット変換も行います。例えばPokéAPIのJSONには英語名や他言語名が含まれる場合がありますが、本パイプラインの用途上必要なければ除外する、あるいはリスト構造をフラットにするなどの処理です。現時点では大きな変換は実装していませんが、Pydanticモデルを導入することで各JSON構造をPythonオブジェクトにマッピングし、不要なフィールドを除去した上でデータベースエントリに変換する下地を整えています。

   （※補足: **Transformフェーズ**は今後充実させる予定の部分であり、現段階では主に「形式変換」（文字列正規化やPythonオブジェクト化）と「関連情報の紐付け準備」（IDマッピング）のみに留まります。）

3. **格納 (Load) - データ保存:**

   取得および変換済みのデータは、**JSONファイルの保存**と**データベースへの保存**という二通りの方法でローカルに格納します。まず、抽出直後のJSONレスポンスは`json/{リソース名}/{ID}.json`としてディスクに書き込まれます。この処理は`fetch_endpoint_for_data()`関数内で行われ、目的のフォルダが無ければ`util/directory.py`の`setup_dir()`で作成してから、`aiofile.AIOFile`を使い**非同期ファイル書き込み**で素早く保存しています（ファイル書き込みも並列で行うことで、I/O待ちによる全体の遅延を抑えます）。保存後、ファイルパスまたは成功フラグを結果リストに追加し、全体で何件成功・失敗したかを集計します。

   次に**データベースへのロード**ですが、現時点では基本的な仕組みのみ実装されています。具体的には、取得データに対応するSQLiteのテーブルを用意し（例: pokemonテーブル、typeテーブル等）、各JSONから主要フィールドを抜粋してテーブルにINSERTする処理を行います。こちらは同期処理で`sqlite3`を使って簡易に行っていますが、将来的には`aiosqlite`やSQLAlchemyの非同期対応を用いて、データ取得と並行してデータベース書き込みも非同期で行えるよう改善予定です。現段階ではパイプライン実行後に別途スクリプト（今後実装予定の`src/load/`モジュール）を実行することで、保存済みJSON群を読み込んでDBに一括登録する運用となっています。

   **格納結果の検証:** パイプライン実行後、コンソールには取得結果のサマリが表示されます。例えば「Saved 1010 JSON files」「Failed 0 JSON files.」のように、全取得対象数と失敗件数が出力されます。エラーになったIDがあればそのIDのファイルは生成されていないため、`tests/test_paths_json.py`などで「想定されたID範囲のJSONファイルがすべて存在するか」をチェックし、漏れがないことを検証します。

以上がETL全体の流れです。まとめると、**PokéAPIから指定リソース（ポケモン関連）の全データを非同期処理で取得→必要なら加工→JSONファイル群として保存→（将来的に）データベースへ統合**というパイプラインになっています。非同期処理により大規模なデータ取得も効率良く行え、ローカルに保存することでAPIアクセスの回数削減やオフライン分析も可能になります。

## セットアップ・インストール手順（Poetry使用前提）

本プロジェクトはPoetryで環境管理されています。以下の手順で開発・実行環境をセットアップできます。

1. **Poetryのインストール:**

   Poetryがインストールされていない場合は、公式手順に従いインストールします（例: `pip install poetry` もしくは公式インストーラの実行）。

   ※PoetryはPython向けのパッケージ管理・ビルドツールで、依存関係の解決と仮想環境の自動作成を行います。

2. **リポジトリのクローン:**

   ターミナルで本プロジェクトを配置したいディレクトリに移動し、`git clone https://github.com/Shiori-Takanashi/poke-pipeline-dev.git` を実行してリポジトリを取得します。

3. **依存関係のインストール:**

   プロジェクトディレクトリ直下（pyproject.tomlがある場所）で以下を実行してください。

   ```bash
   poetry install
   ```

   これにより、Poetryが仮想環境を作成し、必要なパッケージをすべてインストールします。`pyproject.toml`および`poetry.lock`に基づいて環境が再現されます。

   インストールが完了すると、`poetry shell`でその仮想環境に入るか、以下のように`poetry run`で任意のコマンドを実行できます。

4. **環境構築の確認:**

   Poetry経由でPythonを起動し、プロジェクトのモジュールが読み込めることを確認します。例えば:

   ```bash
   poetry run python -c "import src.fetch.fetch_target as ft; print(ft.BASE_URL)"
   ```

   正常に実行できればセットアップ完了です（上記コマンドはBASE\_URLを表示する簡易チェックです）。

もしPoetryを使用しない場合は、代替手順として以下があります（非推奨）:

* 仮想環境を自分で用意し、Python 3.10以上を使用してください。

* プロジェクト直下の`requirements.txt`と`requirements-test.txt`を使い、`pip install -r requirements.txt -r requirements-test.txt`で必要パッケージをインストールします。

* その後、上記Poetryの場合と同様に動作確認してください。

Poetry利用を推奨する理由は、依存関係の整合性やPythonのバージョン管理が容易になるためです。本プロジェクトではpytestや開発ツールも含めPoetryで管理しています。

## 使用方法・コマンド例

セットアップが完了したら、いくつかの方法でパイプラインを実行・検証できます。以下に代表的なコマンド例を示します。

* **1. データ取得パイプラインの実行:**

  PokéAPIから対象データ（デフォルトではポケモン関連データ全て）を取得するには、`fetch_target.py`のメイン処理を走らせます。Poetry経由で次のようにコマンドを実行してください:

  ```bash
  poetry run python src/fetch/fetch_target.py
  ```

  初回実行時は、`json/`ディレクトリ以下にサブフォルダが自動生成され、各種JSONデータの取得が開始されます。コンソールには進捗や結果が表示されます。大量のリクエストを送るため、進捗表示として`rich`や`tqdm`によるプログレスバーが表示されます（**実装中:** 現在簡易な出力のみかもしれませんが、順次改善しています）。完了すると、`Saved XXXX JSON files`といったメッセージが出て、所定のフォルダにファイルが保存されていることを確認できます。

  **所要時間:** 環境にもよりますが、ポケモン約1000件分のデータ取得は並列処理により数十秒程度で完了します（APIサーバやネットワーク状況によって変動）。

* **2. 特定処理のデバッグ実行:**

  例えば取得対象のエンドポイント一覧だけを確認したい場合、`debug_fetch_target.py`を実行できます。

  ```bash
  poetry run python debug/debug_fetch_target.py
  ```

  これは大掛かりな処理はせず、`config`に設定されたBASE\_URLやFETCH\_TARGETを表示し、それに基づく`get_name_and_url()`関数の結果（名前とURLの一覧）を出力します。新たにエンドポイントを追加した際に、正しく認識されているか確認する用途などに使えます。

* **3. テストの実行:**

  コードに変更を加えたり動作を確認したい場合は、用意されたpytestによるテストを実行すると良いでしょう。

  ```bash
  poetry run pytest
  ```

  と実行すれば、`tests/`以下のテストが順次走り、すべてパスすれば現在の実装が意図したとおり動作していることが保証されます。特に非同期処理部分は通常の手動確認が難しいため、テストを通して正しさを検証しています。

  （テスト実行時、データ取得自体はモックまたは最小限のリソースに絞って行われます。一部のテストでは実際にPokéAPIへアクセスするものもありますが、その場合でもlimit=1などを指定して負荷を抑えています。）

* **4. データ内容の確認:**

  データ取得後、その内容を確認したい場合はJSONファイルやSQLite DBを直接見ることができます。

  例えば、ポケモンの基本情報JSONの項目を見るには:

  ```bash
  poetry run python -m json.tool json/pokemon/00025.json | head -20
  ```

  上記は図鑑番号25（ピカチュウ）のJSONファイルを整形表示し、先頭20行を出力する例です。`-m json.tool`を使うことで見やすく出力できます。

  SQLiteデータベースを確認するには、DBブラウザ（SQLite Browser等）で`database/poke.sqlite3`を開くか、SQLiteのコマンドラインを用いてテーブル一覧や件数を確認できます:

  ```bash
  sqlite3 database/poke.sqlite3 "SELECT name, base_experience FROM pokemon WHERE id < 10;"
  ```

  （上記は仮にpokemonテーブルがあり、idと経験値を持っているとした例です。実際のテーブル構造は現在検討中です。）

* **5. CLIインターフェース（将来的な使用法）:**

  将来的には、対話的に操作できるCLIコマンドを導入予定です。その際には、例えば

  ```bash
  poetry run pokecli fetch --resource pokemon --id 25
  ```

  のような形式で特定のポケモンのみ取得したり、

  ```bash
  poetry run pokecli stats --resource pokemon
  ```

  で取得済みデータから統計情報を算出する、といった拡張も視野に入れています。現在はまだCLIスクリプトは整備中ですが、`click`ライブラリは既にインストールされています。

上記のように、本プロジェクトはPoetryで環境を構築し、主に`poetry run ...`で各種コマンドを実行する運用となっています。初回データ取得後は必要に応じて`json/`や`database/`内のデータを活用し、データ分析やアプリケーション組み込みを行ってください。

## 現時点での実装状況と機能一覧

2025年7月現在、本プロジェクトで**実装済みの機能**と**サポートしているデータ**は以下の通りです。

* **PokéAPIデータ取得機能:**

  指定したリソース種別の全データを自動取得する機能が完成しています。非同期HTTP処理＋再試行によって、ネットワークの一時的な失敗にも強い設計です。実装済みの取得対象リソースは**ポケモン関連の主要エンドポイント**です:

  * ポケモン本体 (`pokemon`): 全ポケモンの基本データ（種族値、特性、技一覧など）

  * ポケモンフォーム (`pokemon-form`): フォルム違いのデータ（外見やフォルムごとのステータス差分等）

  * ポケモン種族 (`pokemon-species`): 進化や分類、図鑑テキストなど種族固有のデータ

  * タイプ (`type`): ポケモンのタイプに関するデータ（相性関係やタイプ別ポケモン一覧等）

  * バージョン (`version`): ポケモンゲームのバージョン（例: レッド、サファイアなど）データ

  * バージョングループ (`version-group`): 複数バージョンを含む世代グループ（例: 第1世代、 第2世代...）データ

    これらのデータ取得処理についてはテストも整備されており、例えば**1010体のポケモンを漏れなく取得できること**や、**各JSONに結果が含まれていること**を確認済みです。

* **非同期処理と並列化:**

  `asyncio`と`aiohttp`を組み合わせた非同期並列処理が実装されています。50スレッド相当の並列リクエストで、PokéAPIへのアクセス速度を最大限高めています。また、セマフォによる**同時リクエスト数制御**や、awaitによる**協調的マルチタスク**によって、無限の並列化によるネットワーク詰まりやサーバ過負荷を避けつつ高速化しています。この機能により、大量データの収集時間が大幅に短縮されています。

* **リトライとエラーハンドリング:**

  通信エラーやHTTPエラーコードに対して、一定回数まで再試行する仕組みを備えています。具体的には、各リクエストでネットワークエラー発生時やステータスコード5xx/4xx受信時に、ログを出力しつつ最大5回まで再試行します。5回試してもだめな場合はそのリソースを\*\*失敗（None）\*\*とマークし、処理続行します（最終的に何件失敗したかを集計表示）。これにより、途中で一部のデータ取得が失敗してもパイプライン全体が止まらず、後で失敗分のみ再取得すれば良い状態を保ちます。

* **JSONファイル出力:**

  取得した生データを即座にJSONファイルとして保存する機能があります。エンドポイントごとにディレクトリ分けし、IDごとのファイルに分割保存する実装です。これにより、任意のポケモンのJSONデータにすぐアクセスできます。例えば「フシギダネのタイプを確認したい」といった場合でも、`json/pokemon/00001.json`を開けばよく、わざわざAPIに再アクセスする必要がありません。現在このJSON出力はパイプライン実行中に逐次行っており、**非同期ファイルI/O**でパフォーマンス上のボトルネックとならないよう工夫しています。

* **SQLiteデータベース連携（初期実装）:**

  データを構造化して保存するためのSQLiteデータベース対応があります。現状ではポケモン基本情報やタイプ情報などのテーブルを試験的に作成し、Pythonの`sqlite3`モジュールでINSERTする処理を組み込んでいます。大量データをDBに入れる部分についてはチューニング中ですが、**ユニークキー制約**や**インデックス**の設定なども検討しており、今後の拡張に備えています。データベースに入れることで、例えば「特定タイプのポケモン一覧」「種族値合計が500超のポケモン数」といった**クエリを素早く実行**できるようになります。ただし現在は基本的な項目のみ投入し、深いリレーションまでは張っていません（進化関係など複雑な構造はJSONのまま保持）。これも今後の課題ですが、最低限**全件の基本情報をRDBで扱える**状態にはなっています。

* **ユーティリティ・補助機能:**

  プロジェクトの円滑な開発・実行のための補助機能も実装済みです。

  * `setup_dir()`による**フォルダ自動生成**: 初回実行時に必要なディレクトリ（json各種, database等）を自動で生成します。開発者は空フォルダを手動で作成する手間が省け、クリーンな状態からでもすぐ実行可能です。

  * **エンドポイント名の正規化関数**: 前述の通り、`rename_endpoint_name()`関数でハイフンを含む名前をPythonの識別子兼フォルダ名として使いやすい形に変換します。この関数とテストがあることで、今後PokéAPI側で新たなエンドポイントが追加された場合も容易に対応できます（例: "mega-evolution"というエンドポイントが増えたら"mega\_evolution"に変換）。

  * **デバッグ用スクリプト/ログ**: debugフォルダのスクリプトにより、取得前に設定値が正しいか、リソース一覧が想定通りかを確認できます。また、主要処理には適宜`print`や`logging`で情報を出すようにしており、進行状況やエラーの詳細を把握しやすくしています（本番運用であればloggingレベルで制御しますが、開発中のため適宜標準出力に出す実装です）。

* **テストと品質管理:**

  重要な関数についてはPytestによるテストが記述されており、Travis CIやGitHub Actions等のCIツールと連携すればプッシュ時に自動テストすることも可能です（現状CI設定は未構築）。非同期処理や外部API利用部分のテストには工夫が凝らされており、`pytest-asyncio`マーカーを付けて実際にPokéAPIへ小さなクエリを飛ばしてみるテストを導入しています。またコードスタイル統一のためBlackでの自動フォーマット、型チェックとしてMypyを活用しており、可読性と信頼性の向上に努めています。

以上が現在実装済みの主な機能です。総じて、「PokéAPIからポケモン関連のデータを抜け漏れなく取得し、ローカルに保存する」という基本目標は達成しており、そのための非同期処理・リトライ機構・データ保存方法は一通り揃っています。ただし、**データのさらなる活用（変換・分析）や新機能**については今後の計画として次項で述べます。

## 今後の予定（ToDo や改善点）

本プロジェクトは現在基本的なETLパイプラインが動作する状態ですが、今後下記のような機能追加・改善を予定しています。

* **1. 取得対象リソースの拡充:**

  現在は主にポケモンに関連するエンドポイントのみを対象としていますが、PokéAPIには他にも**技 (move)**、**アイテム (item)**、\*\*特性 (ability)\*\*等、ポケモンデータを語る上で重要なリソースが存在します。これらも`config/target.py`のリストに追加し、同様に取得・保存できるよう対応します。それに伴い、各リソース専用の処理（例えば技データは非常に件数が多いため取得方法を工夫する、など）やデータベースのテーブル追加も行います。

* **2. データベーススキーマとORMの導入:**

  SQLiteへの保存について、現状は最低限のINSERT実装に留まっています。今後は**データベーススキーマ**をしっかり設計し、各テーブル間のリレーションを設定します。例えばポケモンテーブルとタイプテーブルを関連付け、ポケモンのタイプを別テーブルで管理する、ポケモンと進化先をリレーションで紐付ける、といった構造を検討中です。その上でSQLAlchemyのORMを導入し、Pythonオブジェクトとしてデータを扱えるようにします。こうすることで、コード上で`pokemon.types`のように属性にアクセスして関連データをたどることができ、分析やAPI構築が楽になります。また、将来的にSQLiteからPostgreSQL等に切り替える際もORMを使っていれば比較的容易に移行できます。

  さらに、データベースへの書き込みも非同期化（`aiosqlite`やSQLAlchemyのasync対応）し、取得と保存を**同時並行**で行えるよう最適化します。これにより全体の処理時間短縮とコードのシンプル化を図ります。

* **3. データ変換・分析機能の強化:**

  取得したデータに基づいて、より付加価値の高い情報を生成する機能を追加します。例えば:

  * **進化チェーン解析:** ポケモン種族ごとの進化チャートを解析し、ツリー構造をJSONやグラフ画像で出力する。

  * **種族値統計:** 全ポケモンの種族値（HP, 攻撃, 防御...）の分布を算出し、平均値やトップ10を求める。

  * **タイプ相性マトリクス:** タイプ間の相性を二次元表やネットワークグラフ（`networkx`や`pydot`を利用）で可視化する。

  * **世代別追加ポケモン数:** バージョングループごとに新規追加ポケモン数を集計し、世代の規模感を出す。

    こうした分析は本来プロジェクト本筋ではないかもしれませんが、ポートフォリオとしてデータ活用例を示すことでアピールにつながると考えています。実装上は変換フェーズでデータをまとめて算出し、`out/`フォルダに結果を出力するような形にする予定です。

* **4. エラーハンドリングとリカバリ向上:**

  現状でも一定のリトライはありますが、今後さらに堅牢性を高めます。例えば、APIサーバが長時間落ちている場合に備えて**エクスポネンシャルバックオフ**戦略（待機時間を指数的に延ばして再試行）を導入したり、特定IDのデータ取得が連続で失敗する場合にそのIDをスキップしてログに記録、後で別途取得できるようにする、といった処理です。また、中断と再開の仕組みも検討します。一度に全データ取得だと時間がかかるため、途中で停止しても**続きから再開**できるよう、取得済みIDを記録しておく、もしくは既存のJSONがある場合は取得をスキップする、といった工夫を入れる予定です。

* **5. ドキュメンテーションの充実:**

  README（本ドキュメント）の英語版作成や、各モジュールに対する詳細なDocstringコメントの追加を進めます。特にAPIのフィールド説明や、データ項目の意味について補足することで、コードを読む人がPokéAPIの知識が無くても理解できるようにします。またSphinx等を用いてリファレンスドキュメントを生成し、GitHub Pagesで公開することも検討します。これにより、採用担当者がプロジェクト概要だけでなく詳細設計にも触れられるようになります。

* **6. CLI/UIの整備:**

  前述のCLIツールを完成させ、コマンドベースで柔軟に操作できるようにします。例えば`--resource`オプションで取得するリソースを限定したり、`--to-db`フラグで取得後自動でDB登録まで行う、といったものです。加えて、Webダッシュボード的な**簡易UI**も時間があれば検討します。FlaskとChart.js等を組み合わせ、ブラウザから取得データを閲覧・検索できるようにすれば、データエンジニアリングとフロントエンドの両面でスキルを示すことができます。

* **7. コンテナ/Docker対応:**

  環境依存を無くし、誰でも手軽に動かせるようDockerイメージを用意することも視野に入れています。Dockerfileを作成し、Poetry経由で依存をインストールしてエントリポイントでパイプラインを動かす構成です。例えば`docker run poke-pipeline:latest`で一連の処理が実行されるようにしておけば、クローンやPoetry不要で試してもらうことが可能になります。またクラウド上でこのコンテナを動かし、定期的にデータ更新する運用（GitHub ActionsのスケジューラやAWS Batchなどの利用）も将来的に検討します。

以上が主なToDoと改善予定です。特に**データの質的な活用**と**利便性の向上**がテーマになります。これらを順次実装・統合することで、本プロジェクトを**ポケモンデータ解析の包括的プラットフォーム**的な存在にまで発展させることを目指しています。

## ライセンスと謝辞

* **ライセンス:** 本プロジェクトのソースコードはMITライセンスの下で公開する予定です（※現在準備中）。MITライセンスにより、誰でも自由に本プロジェクトのコードを利用・改変・配布できますが、著作権表示とライセンス通知を残す必要があります。ライセンスファイル（LICENSE）は追ってリポジトリに追加します。なお、プロジェクト内で使用している依存ライブラリは各々のライセンスに従います。

* **データに関する注意:** PokéAPIから取得されるポケモンに関するデータおよび画像等の著作権はNintendo/Game Freak他に属します。本プロジェクトはこれら公式データを利用していますが、営利目的ではなくファンコミュニティによるデータ利用です。PokéAPI自体はコミュニティ提供の非公式APIであり、そのAPI仕様および提供データはCreative Commons License（表示-非営利-継承）で公開されています。使用する際はPokéAPIの利用規約に従いましょう。

* **謝辞:** 本プロジェクト開発にあたり、以下のリソース・コミュニティに感謝いたします。

  * **PokéAPI**: 無料で高品質なポケモンデータAPIを提供してくださっているPokéAPIチームに深く感謝します。本プロジェクトの根幹であるデータ取得はPokéAPI無くして成り立ちません。

  * **Python & ライブラリ開発コミュニティ**: asyncioやaiohttpを始めとする強力な非同期処理基盤、pytestやPoetryなどの開発ツールを公開してくださっているコミュニティに感謝します。オープンソースの力により、短期間でこのようなパイプラインを構築できました。

  * **ポケモンコミュニティ**: ポケモンという共通の題材のおかげで楽しくデータエンジニアリングの技術検証ができました。本プロジェクトはポケモンファンかつエンジニアである作者の情熱によるものです。

以上でREADMEは終了です。最後までお読みいただきありがとうございます。本プロジェクトに関するご質問やフィードバックがありましたら、IssueやPull Requestを通じてぜひお寄せください。今後とも改善を重ねてまいりますので、よろしくお願いいたします。
